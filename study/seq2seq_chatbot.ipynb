{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599574102501",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers, losses, metrics\n",
    "from keras import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 입력에 START가 들어가면 디코딩의 시작 의미. 반대로 디코더 출력에 END가 나오면 디코딩 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 단어\n",
    "PAD = \"<PADDING>\"\n",
    "STA = \"<STA>\"\n",
    "END = \"<END>\"\n",
    "OOV = \"<OOV>\"\n",
    "\n",
    "# 태그 인덱스\n",
    "PAD_INDEX = 0\n",
    "STA_INDEX = 1\n",
    "END_INDEX = 2\n",
    "OOV_INDEX = 3\n",
    "\n",
    "# 데이터 타입\n",
    "ENCODER_INPUT = 0\n",
    "DECODER_INPUT  = 1\n",
    "DECODER_TARGET = 2\n",
    "\n",
    "# 한 문장에서 단어 시퀀스의 최대 개수\n",
    "max_sequences = 30\n",
    "\n",
    "# 임베딩 벡터 차원\n",
    "embedding_dim = 100\n",
    "\n",
    "# LSTM 히든 레이어 차원\n",
    "lstm_hidden_dim = 128\n",
    "\n",
    "# 정규 표현식 필터\n",
    "RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "\n",
    "# 챗봇 데이터 로드\n",
    "chatbot_data = pd.read_csv(r'D:\\강의자료\\4 - 딥러닝 자연어처리\\8 - Seq2Seq의 응용\\실습\\dataset\\chatbot\\ChatbotData.csv',encoding='utf-8')\n",
    "question, answer = list(chatbot_data['Q']), list(chatbot_data['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "11823"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Q:12시 땡!\nA:하루가 또 가네요.\n\nQ:1지망 학교 떨어졌어\nA:위로해 드립니다.\n\nQ:3박4일 놀러가고 싶다\nA:여행은 언제나 좋죠.\n\nQ:3박4일 정도 놀러가고 싶다\nA:여행은 언제나 좋죠.\n\nQ:PPL 심하네\nA:눈살이 찌푸려지죠.\n\nQ:SD카드 망가졌어\nA:다시 새로 사는 게 마음 편해요.\n\nQ:SD카드 안돼\nA:다시 새로 사는 게 마음 편해요.\n\nQ:SNS 맞팔 왜 안하지ㅠㅠ\nA:잘 모르고 있을 수도 있어요.\n\nQ:SNS 시간낭비인 거 아는데 매일 하는 중\nA:시간을 정하고 해보세요.\n\nQ:SNS 시간낭비인데 자꾸 보게됨\nA:시간을 정하고 해보세요.\n\n"
    }
   ],
   "source": [
    "# 데이터의 일부만 학습에 사용\n",
    "question = question[:100]\n",
    "answer = answer[:100]\n",
    "\n",
    "# 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q:'+question[i])\n",
    "    print('A:'+answer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 함수 \n",
    "def pos_tag(sentences):\n",
    "    # KoNLPy 형태소 분석기 설정\n",
    "    tagger = Okt()\n",
    "    # 문장 품사 변수 초기화\n",
    "    sentences_pos = []\n",
    "    # 모든 문장 반복\n",
    "    for sentence in sentences:\n",
    "        # 특수기호 제거\n",
    "        sentence = re.sub(RE_FILTER,\"\",sentence)\n",
    "\n",
    "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "        # 형태소 단위로 문자열을 끊고 싶다면, .morphs()를 사용하면 된다\n",
    "        sentence = \" \".join(tagger.morphs(sentence))\n",
    "        sentences_pos.append(sentence)\n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다', '3 박 4일 정도 놀러 가고 싶다', 'PPL 심하네', 'SD 카드 망가졌어', 'SD 카드 안 돼', 'SNS 맞팔 왜 안 하지 ㅠㅠ', 'SNS 시간 낭비 인 거 아는데 매일 하는 중', 'SNS 시간 낭비 인데 자꾸 보게 됨', 'SNS 보면 나 만 빼고 다 행복 해보여', '가끔 궁금해', '가끔 뭐 하는지 궁금해', '가끔 은 혼자 인게 좋다', '가난한 자의 설움', '가만 있어도 땀 난다', '가상 화폐 쫄딱 망함', '가스 불 켜고 나갔어', '가스 불 켜놓고 나온거 같아', '가스 비 너무 많이 나왔다', '가스 비 비싼데 감기 걸리겠어', '가스 비 장난 아님', '가장 확실한 건 뭘 까', '가족 여행 가기 로 했어', '가족 여행 고고', '가족 여행 어디 로 가지', '가족 있어', '가족 관계 알려 줘', '가족 끼리 여행 간다', '가족 들 보고 싶어', '가족 들 이랑 서먹해', '가족 들 이랑 서먹해졌어', '가족 들 이랑 어디 가지', '가족 들 이랑 여행 갈거야', '가족 여행 가야 지', '가족 이 누구 야', '가족 이랑 여행 가려고', '가족 한테 스트레스 풀었어', '가출 할까', '가출 해도 갈 데 가 없어', '간만 에 떨리니까 좋더라', '간만 에 쇼핑 중', '간만 에 휴식 중', '간식 뭐 먹을까', '간식 추천', '간장 치킨 시켜야지', '간접흡연 싫어', '갈까 말까 고민 돼', '갈까 말까', '감 말랭이 먹고 싶다', '감 말랭이 먹어야지', '감기 같 애', '감기 걸린 것 같아', '감기 기운 이 있어', '감기 들 거 같 애', '감기 가 오려나', '감기 약 이 없어', '감기 인거 같 애', '감미로운 목소리 좋아', '감정 이 쓰레기통 처럼 엉망 진창 이야', '감정 컨트롤 을 못 하겠어', '감정 컨트롤 이 안 돼', '감히 나를 무시 하는 애가 있어', '갑자기 나쁜 생각 이 막 들더라', '갑자기 눈물 나', '갑자기 물어봐서 당황 했어', '갑자기 불편한 사이 가 된 거 같아', '강렬한 첫인상 남겨야 하는데', '강아지 키우고 싶어', '강아지 키우고 싶은데 역시 안 돼겠지', '강아지 키울 수 있을까', '강아지 키울까', '강원도 가서 살까', '같이 게임 하자고 해도 되나', '같이 놀러 갈 친구 가 없어', '같이 먹었는데 나 만 살찐 거 같아', '같이 수영장 가기 로 했어', '같이 있으면 힘든데 붙잡고 싶어', '같이 피 씨방 가자고 해볼까', '같이 할 수 있는 취미 생활 뭐 있을까', '개강 룩 입어볼까', '개강 옷 예쁘게 입어 볼까', '개강 이다', '개강 이라니', '개 같은 상황', '개 같이 되 버렸어', '개 기름 꼈어', '개념 도 놓고 옴', '개념 이 없어', '개 당황', '개 당황 했잖아 갑자기 물어 봐서', '개인 적 인 업무 까지 다 시켜', '개인 적 인 일도 다 시켜', '개 졸려', '개 좋아', '개학 하 니까 좋다', '걔 너무 싫다', '걔 는 누굴 닮아서 그런거니', '걔 랑 같은 반 됐으면 좋겠다', '거지 같이 일해 놓고 갔어']\n['하루 가 또 가네요', '위로 해 드립니다', '여행 은 언제나 좋죠', '여행 은 언제나 좋죠', '눈살 이 찌푸려지죠', '다시 새로 사는 게 마음 편해요', '다시 새로 사는 게 마음 편해요', '잘 모르고 있을 수도 있어요', '시간 을 정 하고 해보세요', '시간 을 정 하고 해보세요', '자랑 하는 자리 니까 요', '그 사람 도 그럴 거 예요', '그 사람 도 그럴 거 예요', '혼자 를 즐기세요', '돈 은 다시 들어올 거 예요', '땀 을 식혀주세요', '어서 잊고 새 출발 하세요', '빨리 집 에 돌아가서 끄고 나오세요', '빨리 집 에 돌아가서 끄고 나오세요', '다음 달 에는 더 절약 해봐요', '따뜻하게 사세요', '다음 달 에는 더 절약 해봐요', '가장 확실한 시간 은 오늘이 에요 어제 와 내일 을 놓고 고민 하느라 시간 을 낭비하지 마세요', '온 가족 이 모두 마음 에 드는 곳 으로 가보세요', '온 가족 이 모두 마음 에 드는 곳 으로 가보세요', '온 가족 이 모두 마음 에 드는 곳 으로 가보세요', '저 를 만들어 준 사람 을 부모님 저 랑 이야기 해 주는 사람 을 친구 로 생각 하고 있어요', '저 를 만들어 준 사람 을 부모님 저 랑 이야기 해 주는 사람 을 친구 로 생각 하고 있어요', '더 가까워질 기회 가 되겠네요', '저 도 요', '다 들 바빠서 이야기 할 시간 이 부족했나 봐요', '다 들 바빠서 이야기 할 시간 이 부족했나 봐요', '온 가족 이 모두 마음 에 드는 곳 으로 가보세요', '좋은 생각 이에요', '더 가까워질 기회 가 되겠네요', '저 를 만들어 준 사람 을 부모님 저 랑 이야기 해 주는 사람 을 친구 로 생각 하고 있어요', '좋은 생각 이에요', '정말 후회 할 습관 이에요', '무모한 결정 을 내 리지 마세요', '선생님 이나 기관 에 연락 해보세요', '떨리는 감정 은 그 자체 로 소중해요', '득템 했길 바라요', '휴식 도 필요하죠', '단 짠으로 두 개 사는게 진리 죠', '단 짠으로 두 개 사는게 진리 죠', '맛있게 드세요', '저 도 싫어요', '가세 요', '가세 요', '맛있게 드세요', '맛있게 드세요', '병원 가세 요', '이럴 때 잘 쉬는 게 중요해요', '이럴 때 잘 쉬는 게 중요해요', '이럴 때 잘 쉬는 게 중요해요', '따뜻하게 관리 하세요', '병원 가세 요', '병원 가세 요', '저 도 듣고 싶네요', '자신 을 더 사랑 해주세요', '그건 습관 이에요', '그건 습관 이에요', '콕 집어서 물어보세요', '좋은 생각 만 하세요', '마음 이 아픈가요', '갑작스러웠나 봐요', '관계 의 변화 가 왔나 봅니다', '처음 3초 가 중요해요 당신 의 매력 을 어필 해보세요', '책임질 수 있을 때 키워 보세요', '먼저 생활 패턴 을 살펴 보세요', '먼저 생활 패턴 을 살펴 보세요', '책임질 수 있을 때 키워 보세요', '아름다운 곳 이 죠', '안 될 것 도 없죠', '혼자 도 좋아요', '연인 은 살쪄도 잘 알아차리지 못 하고 알아차려도 싫어하지 않을 거 예요', '즐거운 시간 보내고 오세요', '질질 끌 지 마세요', '말 해보세요', '함께 하면 서로 를 더 많이 알 게 될 거 예요', '개시 해보세요', '개시 해보세요', '곧 방학 이 예요', '방학 이 참 짧죠', '벗어나는 게 좋겠네요', '벗어나는 게 좋겠네요', '세수 하고 오세요', '그게 제일 중요한 건데 요', '그게 제일 중요한 건데 요', '다음 부터는 더 많이 아세요', '갑작스러웠나 봐요', '공적 인 일 부터 하세요', '공적 인 일 부터 하세요', '낮잠 을 잠깐 자도 괜찮아요', '저 도 좋아해주세요', '친구 들 이 보고싶었나 봐요', '되도록 만나지 마세요', '당신 이 요', '당신 의 운 을 믿어 보세요', '일 못 하는 사람 이 있으면 옆 에 있는 사람 이 더 힘들죠']\nQ:12시 땡\nA:하루 가 또 가네요\n\nQ:1 지망 학교 떨어졌어\nA:위로 해 드립니다\n\nQ:3 박 4일 놀러 가고 싶다\nA:여행 은 언제나 좋죠\n\nQ:3 박 4일 정도 놀러 가고 싶다\nA:여행 은 언제나 좋죠\n\nQ:PPL 심하네\nA:눈살 이 찌푸려지죠\n\nQ:SD 카드 망가졌어\nA:다시 새로 사는 게 마음 편해요\n\nQ:SD 카드 안 돼\nA:다시 새로 사는 게 마음 편해요\n\nQ:SNS 맞팔 왜 안 하지 ㅠㅠ\nA:잘 모르고 있을 수도 있어요\n\nQ:SNS 시간 낭비 인 거 아는데 매일 하는 중\nA:시간 을 정 하고 해보세요\n\nQ:SNS 시간 낭비 인데 자꾸 보게 됨\nA:시간 을 정 하고 해보세요\n\n"
    }
   ],
   "source": [
    "# 형태소 분석 수행\n",
    "question = pos_tag(question)\n",
    "answer = pos_tag(answer)\n",
    "print(question)\n",
    "print(answer)\n",
    "# 형태소 분석으로 변환된 챗봇 데이터 출력\n",
    "for i in range(10):\n",
    "    print('Q:' + question[i])\n",
    "    print('A:' + answer[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 대답 문장들을 하나로 합침\n",
    "sentences=[]\n",
    "sentences.extend(question)\n",
    "sentences.extend(answer)\n",
    "\n",
    "words=[]\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        words.append(word)\n",
    "# 길이가 0인 단어는 삭제\n",
    "words = [word for word in words if len(word)>0]\n",
    "\n",
    "# 중복된 단어 삭제\n",
    "words = list(set(words))\n",
    "\n",
    "# 제일 앞에 태그 단어 삽입\n",
    "words[:0] = [PAD, STA, END, OOV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질문과 대답 문장들을 합쳐서 전체 단어사전 만들기. 자연어 처리에서는 항상 이렇게 단어를 인덱스에 따라서 정리.그래야지 문장을 인덱스 배열로 바꿔서 임베딩 레이어에 넣을 수 있습니다. \n",
    "또한 모델의 출력에서 나온 인덱스를 다시 단어로 변환하는데도 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "454"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 단어 개수\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['<PADDING>',\n '<STA>',\n '<END>',\n '<OOV>',\n '싫다',\n '가난한',\n '예쁘게',\n '에요',\n '보고',\n '싫어하지',\n '돼겠지',\n '돈',\n '일도',\n '살펴',\n '먹을까',\n '좋아',\n '고고',\n '좋겠다',\n '연인',\n '아님',\n '엉망',\n '되나',\n '개시',\n '낭비하지',\n '와',\n '키워',\n '나쁜',\n '망가졌어',\n '낭비',\n '빼고',\n '가고',\n '끄고',\n '없어',\n '로',\n '끼리',\n '룩',\n '후회',\n '치킨',\n '해도',\n '당신',\n '시간',\n '맛있게',\n '데',\n '나오세요',\n '사랑',\n '옴',\n '야',\n '참',\n '비',\n '학교',\n '싶어',\n '부터는',\n '키울까',\n '싶다',\n '인데',\n '기름',\n '알려',\n '또',\n '해보여',\n '자의',\n '눈살',\n '먹고',\n '새로',\n '내',\n '키우고',\n '하느라',\n '그',\n '3초',\n '는',\n '남겨야',\n '감',\n '달',\n '이럴',\n '아세요',\n '소중해요',\n '못',\n '가상',\n '게임',\n '들더라',\n '결정',\n '난다',\n '보게',\n '인게',\n '불편한',\n '입어',\n '닮아서',\n '중',\n '정도',\n '생활',\n '먹었는데',\n '쓰레기통',\n '뭐',\n '변화',\n '물어',\n '부모님',\n '확실한',\n '봅니다',\n '식혀주세요',\n '게',\n '약',\n '입어볼까',\n '된',\n '하는지',\n '되겠네요',\n '끌',\n '갈까',\n '중요해요',\n '이',\n '습관',\n '선생님',\n '힘들죠',\n '하지',\n '아름다운',\n '그런거니',\n '듣고',\n '간다',\n '줘',\n '보세요',\n '그건',\n '할',\n '나',\n '감정',\n '생각',\n '요',\n '말',\n '사이',\n '쫄딱',\n '같아',\n '땀',\n '망함',\n '서로',\n 'PPL',\n '주는',\n '박',\n '씨방',\n '도',\n '가보세요',\n '하고',\n '이다',\n '업무',\n '모두',\n '낮잠',\n '집어서',\n '무시',\n '운',\n '되도록',\n '진창',\n '오세요',\n '오려나',\n '목소리',\n '바라요',\n '것',\n '갑작스러웠나',\n '개념',\n '해볼까',\n '연락',\n '다음',\n '할까',\n '간만',\n '오늘이',\n '혼자',\n '없죠',\n '반',\n '먹어야지',\n '가만',\n '부족했나',\n '화폐',\n '해보세요',\n '간식',\n '있어',\n '기회',\n '많이',\n '개인',\n '수영장',\n '3',\n '쉬는',\n '강원도',\n '좋죠',\n '될',\n '봐서',\n '알아차리지',\n '방학',\n '땡',\n '사는',\n '행복',\n '은',\n '자리',\n '두',\n '놀러',\n '이랑',\n '당황',\n '여행',\n '스트레스',\n '가족',\n '까지',\n '시켜',\n '제일',\n '감히',\n '했길',\n '하면',\n '좋겠네요',\n '인',\n '켜고',\n '마세요',\n '콕',\n '4일',\n '인거',\n '나를',\n '간접흡연',\n '컨트롤',\n '이야',\n '가지',\n 'SNS',\n '준',\n '자랑',\n '짠으로',\n '다시',\n '수도',\n '개학',\n '떨리니까',\n '에는',\n '있을까',\n '이에요',\n '죠',\n '중요한',\n '기관',\n '되',\n '마음',\n '돌아가서',\n '맞팔',\n '가기',\n '갔어',\n '자도',\n '건',\n '말랭이',\n '않을',\n '하겠어',\n '풀었어',\n '가끔',\n '간장',\n '싫어요',\n '개강',\n '사람',\n '됐으면',\n '싶은데',\n '알아차려도',\n '가세',\n '같',\n '나왔다',\n '누구',\n '있어요',\n '시켜야지',\n '걔',\n '서먹해졌어',\n '질질',\n '니까',\n '물어보세요',\n '함께',\n 'SD',\n '책임질',\n '12시',\n '설움',\n '누굴',\n '1',\n '잠깐',\n '하는',\n '관리',\n '불',\n '그게',\n '즐거운',\n '적',\n '떨리는',\n '하세요',\n '다',\n '잘',\n '떨어졌어',\n '사는게',\n '짧죠',\n '걸리겠어',\n '하루',\n '옷',\n '일해',\n '즐기세요',\n '나온거',\n '보고싶었나',\n '드세요',\n '싫어',\n '쇼핑',\n '벗어나는',\n '해봐요',\n '나갔어',\n '강렬한',\n '한테',\n '저',\n '세수',\n '드는',\n '무모한',\n '득템',\n '수',\n '출발',\n '있으면',\n '가려고',\n '강아지',\n '있는',\n '내일',\n '어디',\n '서먹해',\n '어필',\n '어서',\n '첫인상',\n '안',\n '갈',\n '붙잡고',\n '빨리',\n '보내고',\n '더',\n '장난',\n '졸려',\n '들어올',\n '심하네',\n '자체',\n '갈거야',\n '예요',\n '상황',\n '뭘',\n '추천',\n '지',\n '매일',\n '역시',\n '드립니다',\n '온',\n '카드',\n '자꾸',\n '위로',\n '을',\n '힘든데',\n '가네요',\n '아픈가요',\n '가까워질',\n '버렸어',\n '있어도',\n '보면',\n '눈물',\n '랑',\n 'ㅠㅠ',\n '를',\n '고민',\n '물어봐서',\n '모르고',\n '필요하죠',\n '관계',\n '처럼',\n '아는데',\n '살까',\n '꼈어',\n '괜찮아요',\n '이나',\n '의',\n '하자고',\n '하',\n '잊고',\n '가자고',\n '왔나',\n '처음',\n '봐요',\n '갑자기',\n '찌푸려지죠',\n '절약',\n '그럴',\n '만들어',\n '거지',\n '가서',\n '하는데',\n '으로',\n '가스',\n '이야기',\n '싶네요',\n '알',\n '돼',\n '했어',\n '만',\n '옆',\n '살쪄도',\n '공적',\n '먼저',\n '편해요',\n '좋아해주세요',\n '진리',\n '막',\n '했잖아',\n '때',\n '믿어',\n '좋아요',\n '됨',\n '새',\n '애',\n '좋은',\n '기운',\n '감미로운',\n '병원',\n '켜놓고',\n '사세요',\n '왜',\n '볼까',\n '어제',\n '리지',\n '가야',\n '매력',\n '정말',\n '너무',\n '취미',\n '가장',\n '언제나',\n '정',\n '패턴',\n '이라니',\n '거',\n '곧',\n '있을',\n '건데',\n '바빠서',\n '좋다',\n '부터',\n '궁금해',\n '만나지',\n '해주세요',\n '가출',\n '키울',\n '피',\n '단',\n '에',\n '일',\n '말까',\n '개',\n '가',\n '들',\n '애가',\n '놓고',\n '따뜻하게',\n '감기',\n '휴식',\n '걸린',\n '좋더라',\n '해',\n '지망',\n '친구',\n '자신',\n '까',\n '같이',\n '같은',\n '곳',\n '집',\n '살찐',\n '비싼데']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# 단어 출력\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 인덱스의 딕셔너리 생성\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "index_to_word = {index: word for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'<PADDING>': 0,\n '<STA>': 1,\n '<END>': 2,\n '<OOV>': 3,\n '싫다': 4,\n '가난한': 5,\n '예쁘게': 6,\n '에요': 7,\n '보고': 8,\n '싫어하지': 9,\n '돼겠지': 10,\n '돈': 11,\n '일도': 12,\n '살펴': 13,\n '먹을까': 14,\n '좋아': 15,\n '고고': 16,\n '좋겠다': 17,\n '연인': 18,\n '아님': 19,\n '엉망': 20,\n '되나': 21,\n '개시': 22,\n '낭비하지': 23,\n '와': 24,\n '키워': 25,\n '나쁜': 26,\n '망가졌어': 27,\n '낭비': 28,\n '빼고': 29,\n '가고': 30,\n '끄고': 31,\n '없어': 32,\n '로': 33,\n '끼리': 34,\n '룩': 35,\n '후회': 36,\n '치킨': 37,\n '해도': 38,\n '당신': 39,\n '시간': 40,\n '맛있게': 41,\n '데': 42,\n '나오세요': 43,\n '사랑': 44,\n '옴': 45,\n '야': 46,\n '참': 47,\n '비': 48,\n '학교': 49,\n '싶어': 50,\n '부터는': 51,\n '키울까': 52,\n '싶다': 53,\n '인데': 54,\n '기름': 55,\n '알려': 56,\n '또': 57,\n '해보여': 58,\n '자의': 59,\n '눈살': 60,\n '먹고': 61,\n '새로': 62,\n '내': 63,\n '키우고': 64,\n '하느라': 65,\n '그': 66,\n '3초': 67,\n '는': 68,\n '남겨야': 69,\n '감': 70,\n '달': 71,\n '이럴': 72,\n '아세요': 73,\n '소중해요': 74,\n '못': 75,\n '가상': 76,\n '게임': 77,\n '들더라': 78,\n '결정': 79,\n '난다': 80,\n '보게': 81,\n '인게': 82,\n '불편한': 83,\n '입어': 84,\n '닮아서': 85,\n '중': 86,\n '정도': 87,\n '생활': 88,\n '먹었는데': 89,\n '쓰레기통': 90,\n '뭐': 91,\n '변화': 92,\n '물어': 93,\n '부모님': 94,\n '확실한': 95,\n '봅니다': 96,\n '식혀주세요': 97,\n '게': 98,\n '약': 99,\n '입어볼까': 100,\n '된': 101,\n '하는지': 102,\n '되겠네요': 103,\n '끌': 104,\n '갈까': 105,\n '중요해요': 106,\n '이': 107,\n '습관': 108,\n '선생님': 109,\n '힘들죠': 110,\n '하지': 111,\n '아름다운': 112,\n '그런거니': 113,\n '듣고': 114,\n '간다': 115,\n '줘': 116,\n '보세요': 117,\n '그건': 118,\n '할': 119,\n '나': 120,\n '감정': 121,\n '생각': 122,\n '요': 123,\n '말': 124,\n '사이': 125,\n '쫄딱': 126,\n '같아': 127,\n '땀': 128,\n '망함': 129,\n '서로': 130,\n 'PPL': 131,\n '주는': 132,\n '박': 133,\n '씨방': 134,\n '도': 135,\n '가보세요': 136,\n '하고': 137,\n '이다': 138,\n '업무': 139,\n '모두': 140,\n '낮잠': 141,\n '집어서': 142,\n '무시': 143,\n '운': 144,\n '되도록': 145,\n '진창': 146,\n '오세요': 147,\n '오려나': 148,\n '목소리': 149,\n '바라요': 150,\n '것': 151,\n '갑작스러웠나': 152,\n '개념': 153,\n '해볼까': 154,\n '연락': 155,\n '다음': 156,\n '할까': 157,\n '간만': 158,\n '오늘이': 159,\n '혼자': 160,\n '없죠': 161,\n '반': 162,\n '먹어야지': 163,\n '가만': 164,\n '부족했나': 165,\n '화폐': 166,\n '해보세요': 167,\n '간식': 168,\n '있어': 169,\n '기회': 170,\n '많이': 171,\n '개인': 172,\n '수영장': 173,\n '3': 174,\n '쉬는': 175,\n '강원도': 176,\n '좋죠': 177,\n '될': 178,\n '봐서': 179,\n '알아차리지': 180,\n '방학': 181,\n '땡': 182,\n '사는': 183,\n '행복': 184,\n '은': 185,\n '자리': 186,\n '두': 187,\n '놀러': 188,\n '이랑': 189,\n '당황': 190,\n '여행': 191,\n '스트레스': 192,\n '가족': 193,\n '까지': 194,\n '시켜': 195,\n '제일': 196,\n '감히': 197,\n '했길': 198,\n '하면': 199,\n '좋겠네요': 200,\n '인': 201,\n '켜고': 202,\n '마세요': 203,\n '콕': 204,\n '4일': 205,\n '인거': 206,\n '나를': 207,\n '간접흡연': 208,\n '컨트롤': 209,\n '이야': 210,\n '가지': 211,\n 'SNS': 212,\n '준': 213,\n '자랑': 214,\n '짠으로': 215,\n '다시': 216,\n '수도': 217,\n '개학': 218,\n '떨리니까': 219,\n '에는': 220,\n '있을까': 221,\n '이에요': 222,\n '죠': 223,\n '중요한': 224,\n '기관': 225,\n '되': 226,\n '마음': 227,\n '돌아가서': 228,\n '맞팔': 229,\n '가기': 230,\n '갔어': 231,\n '자도': 232,\n '건': 233,\n '말랭이': 234,\n '않을': 235,\n '하겠어': 236,\n '풀었어': 237,\n '가끔': 238,\n '간장': 239,\n '싫어요': 240,\n '개강': 241,\n '사람': 242,\n '됐으면': 243,\n '싶은데': 244,\n '알아차려도': 245,\n '가세': 246,\n '같': 247,\n '나왔다': 248,\n '누구': 249,\n '있어요': 250,\n '시켜야지': 251,\n '걔': 252,\n '서먹해졌어': 253,\n '질질': 254,\n '니까': 255,\n '물어보세요': 256,\n '함께': 257,\n 'SD': 258,\n '책임질': 259,\n '12시': 260,\n '설움': 261,\n '누굴': 262,\n '1': 263,\n '잠깐': 264,\n '하는': 265,\n '관리': 266,\n '불': 267,\n '그게': 268,\n '즐거운': 269,\n '적': 270,\n '떨리는': 271,\n '하세요': 272,\n '다': 273,\n '잘': 274,\n '떨어졌어': 275,\n '사는게': 276,\n '짧죠': 277,\n '걸리겠어': 278,\n '하루': 279,\n '옷': 280,\n '일해': 281,\n '즐기세요': 282,\n '나온거': 283,\n '보고싶었나': 284,\n '드세요': 285,\n '싫어': 286,\n '쇼핑': 287,\n '벗어나는': 288,\n '해봐요': 289,\n '나갔어': 290,\n '강렬한': 291,\n '한테': 292,\n '저': 293,\n '세수': 294,\n '드는': 295,\n '무모한': 296,\n '득템': 297,\n '수': 298,\n '출발': 299,\n '있으면': 300,\n '가려고': 301,\n '강아지': 302,\n '있는': 303,\n '내일': 304,\n '어디': 305,\n '서먹해': 306,\n '어필': 307,\n '어서': 308,\n '첫인상': 309,\n '안': 310,\n '갈': 311,\n '붙잡고': 312,\n '빨리': 313,\n '보내고': 314,\n '더': 315,\n '장난': 316,\n '졸려': 317,\n '들어올': 318,\n '심하네': 319,\n '자체': 320,\n '갈거야': 321,\n '예요': 322,\n '상황': 323,\n '뭘': 324,\n '추천': 325,\n '지': 326,\n '매일': 327,\n '역시': 328,\n '드립니다': 329,\n '온': 330,\n '카드': 331,\n '자꾸': 332,\n '위로': 333,\n '을': 334,\n '힘든데': 335,\n '가네요': 336,\n '아픈가요': 337,\n '가까워질': 338,\n '버렸어': 339,\n '있어도': 340,\n '보면': 341,\n '눈물': 342,\n '랑': 343,\n 'ㅠㅠ': 344,\n '를': 345,\n '고민': 346,\n '물어봐서': 347,\n '모르고': 348,\n '필요하죠': 349,\n '관계': 350,\n '처럼': 351,\n '아는데': 352,\n '살까': 353,\n '꼈어': 354,\n '괜찮아요': 355,\n '이나': 356,\n '의': 357,\n '하자고': 358,\n '하': 359,\n '잊고': 360,\n '가자고': 361,\n '왔나': 362,\n '처음': 363,\n '봐요': 364,\n '갑자기': 365,\n '찌푸려지죠': 366,\n '절약': 367,\n '그럴': 368,\n '만들어': 369,\n '거지': 370,\n '가서': 371,\n '하는데': 372,\n '으로': 373,\n '가스': 374,\n '이야기': 375,\n '싶네요': 376,\n '알': 377,\n '돼': 378,\n '했어': 379,\n '만': 380,\n '옆': 381,\n '살쪄도': 382,\n '공적': 383,\n '먼저': 384,\n '편해요': 385,\n '좋아해주세요': 386,\n '진리': 387,\n '막': 388,\n '했잖아': 389,\n '때': 390,\n '믿어': 391,\n '좋아요': 392,\n '됨': 393,\n '새': 394,\n '애': 395,\n '좋은': 396,\n '기운': 397,\n '감미로운': 398,\n '병원': 399,\n '켜놓고': 400,\n '사세요': 401,\n '왜': 402,\n '볼까': 403,\n '어제': 404,\n '리지': 405,\n '가야': 406,\n '매력': 407,\n '정말': 408,\n '너무': 409,\n '취미': 410,\n '가장': 411,\n '언제나': 412,\n '정': 413,\n '패턴': 414,\n '이라니': 415,\n '거': 416,\n '곧': 417,\n '있을': 418,\n '건데': 419,\n '바빠서': 420,\n '좋다': 421,\n '부터': 422,\n '궁금해': 423,\n '만나지': 424,\n '해주세요': 425,\n '가출': 426,\n '키울': 427,\n '피': 428,\n '단': 429,\n '에': 430,\n '일': 431,\n '말까': 432,\n '개': 433,\n '가': 434,\n '들': 435,\n '애가': 436,\n '놓고': 437,\n '따뜻하게': 438,\n '감기': 439,\n '휴식': 440,\n '걸린': 441,\n '좋더라': 442,\n '해': 443,\n '지망': 444,\n '친구': 445,\n '자신': 446,\n '까': 447,\n '같이': 448,\n '같은': 449,\n '곳': 450,\n '집': 451,\n '살찐': 452,\n '비싼데': 453}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# 단어->인덱스\n",
    "# 문장을 인덱스로 변환하여 모델 입력으로 사용\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{0: '<PADDING>',\n 1: '<STA>',\n 2: '<END>',\n 3: '<OOV>',\n 4: '싫다',\n 5: '가난한',\n 6: '예쁘게',\n 7: '에요',\n 8: '보고',\n 9: '싫어하지',\n 10: '돼겠지',\n 11: '돈',\n 12: '일도',\n 13: '살펴',\n 14: '먹을까',\n 15: '좋아',\n 16: '고고',\n 17: '좋겠다',\n 18: '연인',\n 19: '아님',\n 20: '엉망',\n 21: '되나',\n 22: '개시',\n 23: '낭비하지',\n 24: '와',\n 25: '키워',\n 26: '나쁜',\n 27: '망가졌어',\n 28: '낭비',\n 29: '빼고',\n 30: '가고',\n 31: '끄고',\n 32: '없어',\n 33: '로',\n 34: '끼리',\n 35: '룩',\n 36: '후회',\n 37: '치킨',\n 38: '해도',\n 39: '당신',\n 40: '시간',\n 41: '맛있게',\n 42: '데',\n 43: '나오세요',\n 44: '사랑',\n 45: '옴',\n 46: '야',\n 47: '참',\n 48: '비',\n 49: '학교',\n 50: '싶어',\n 51: '부터는',\n 52: '키울까',\n 53: '싶다',\n 54: '인데',\n 55: '기름',\n 56: '알려',\n 57: '또',\n 58: '해보여',\n 59: '자의',\n 60: '눈살',\n 61: '먹고',\n 62: '새로',\n 63: '내',\n 64: '키우고',\n 65: '하느라',\n 66: '그',\n 67: '3초',\n 68: '는',\n 69: '남겨야',\n 70: '감',\n 71: '달',\n 72: '이럴',\n 73: '아세요',\n 74: '소중해요',\n 75: '못',\n 76: '가상',\n 77: '게임',\n 78: '들더라',\n 79: '결정',\n 80: '난다',\n 81: '보게',\n 82: '인게',\n 83: '불편한',\n 84: '입어',\n 85: '닮아서',\n 86: '중',\n 87: '정도',\n 88: '생활',\n 89: '먹었는데',\n 90: '쓰레기통',\n 91: '뭐',\n 92: '변화',\n 93: '물어',\n 94: '부모님',\n 95: '확실한',\n 96: '봅니다',\n 97: '식혀주세요',\n 98: '게',\n 99: '약',\n 100: '입어볼까',\n 101: '된',\n 102: '하는지',\n 103: '되겠네요',\n 104: '끌',\n 105: '갈까',\n 106: '중요해요',\n 107: '이',\n 108: '습관',\n 109: '선생님',\n 110: '힘들죠',\n 111: '하지',\n 112: '아름다운',\n 113: '그런거니',\n 114: '듣고',\n 115: '간다',\n 116: '줘',\n 117: '보세요',\n 118: '그건',\n 119: '할',\n 120: '나',\n 121: '감정',\n 122: '생각',\n 123: '요',\n 124: '말',\n 125: '사이',\n 126: '쫄딱',\n 127: '같아',\n 128: '땀',\n 129: '망함',\n 130: '서로',\n 131: 'PPL',\n 132: '주는',\n 133: '박',\n 134: '씨방',\n 135: '도',\n 136: '가보세요',\n 137: '하고',\n 138: '이다',\n 139: '업무',\n 140: '모두',\n 141: '낮잠',\n 142: '집어서',\n 143: '무시',\n 144: '운',\n 145: '되도록',\n 146: '진창',\n 147: '오세요',\n 148: '오려나',\n 149: '목소리',\n 150: '바라요',\n 151: '것',\n 152: '갑작스러웠나',\n 153: '개념',\n 154: '해볼까',\n 155: '연락',\n 156: '다음',\n 157: '할까',\n 158: '간만',\n 159: '오늘이',\n 160: '혼자',\n 161: '없죠',\n 162: '반',\n 163: '먹어야지',\n 164: '가만',\n 165: '부족했나',\n 166: '화폐',\n 167: '해보세요',\n 168: '간식',\n 169: '있어',\n 170: '기회',\n 171: '많이',\n 172: '개인',\n 173: '수영장',\n 174: '3',\n 175: '쉬는',\n 176: '강원도',\n 177: '좋죠',\n 178: '될',\n 179: '봐서',\n 180: '알아차리지',\n 181: '방학',\n 182: '땡',\n 183: '사는',\n 184: '행복',\n 185: '은',\n 186: '자리',\n 187: '두',\n 188: '놀러',\n 189: '이랑',\n 190: '당황',\n 191: '여행',\n 192: '스트레스',\n 193: '가족',\n 194: '까지',\n 195: '시켜',\n 196: '제일',\n 197: '감히',\n 198: '했길',\n 199: '하면',\n 200: '좋겠네요',\n 201: '인',\n 202: '켜고',\n 203: '마세요',\n 204: '콕',\n 205: '4일',\n 206: '인거',\n 207: '나를',\n 208: '간접흡연',\n 209: '컨트롤',\n 210: '이야',\n 211: '가지',\n 212: 'SNS',\n 213: '준',\n 214: '자랑',\n 215: '짠으로',\n 216: '다시',\n 217: '수도',\n 218: '개학',\n 219: '떨리니까',\n 220: '에는',\n 221: '있을까',\n 222: '이에요',\n 223: '죠',\n 224: '중요한',\n 225: '기관',\n 226: '되',\n 227: '마음',\n 228: '돌아가서',\n 229: '맞팔',\n 230: '가기',\n 231: '갔어',\n 232: '자도',\n 233: '건',\n 234: '말랭이',\n 235: '않을',\n 236: '하겠어',\n 237: '풀었어',\n 238: '가끔',\n 239: '간장',\n 240: '싫어요',\n 241: '개강',\n 242: '사람',\n 243: '됐으면',\n 244: '싶은데',\n 245: '알아차려도',\n 246: '가세',\n 247: '같',\n 248: '나왔다',\n 249: '누구',\n 250: '있어요',\n 251: '시켜야지',\n 252: '걔',\n 253: '서먹해졌어',\n 254: '질질',\n 255: '니까',\n 256: '물어보세요',\n 257: '함께',\n 258: 'SD',\n 259: '책임질',\n 260: '12시',\n 261: '설움',\n 262: '누굴',\n 263: '1',\n 264: '잠깐',\n 265: '하는',\n 266: '관리',\n 267: '불',\n 268: '그게',\n 269: '즐거운',\n 270: '적',\n 271: '떨리는',\n 272: '하세요',\n 273: '다',\n 274: '잘',\n 275: '떨어졌어',\n 276: '사는게',\n 277: '짧죠',\n 278: '걸리겠어',\n 279: '하루',\n 280: '옷',\n 281: '일해',\n 282: '즐기세요',\n 283: '나온거',\n 284: '보고싶었나',\n 285: '드세요',\n 286: '싫어',\n 287: '쇼핑',\n 288: '벗어나는',\n 289: '해봐요',\n 290: '나갔어',\n 291: '강렬한',\n 292: '한테',\n 293: '저',\n 294: '세수',\n 295: '드는',\n 296: '무모한',\n 297: '득템',\n 298: '수',\n 299: '출발',\n 300: '있으면',\n 301: '가려고',\n 302: '강아지',\n 303: '있는',\n 304: '내일',\n 305: '어디',\n 306: '서먹해',\n 307: '어필',\n 308: '어서',\n 309: '첫인상',\n 310: '안',\n 311: '갈',\n 312: '붙잡고',\n 313: '빨리',\n 314: '보내고',\n 315: '더',\n 316: '장난',\n 317: '졸려',\n 318: '들어올',\n 319: '심하네',\n 320: '자체',\n 321: '갈거야',\n 322: '예요',\n 323: '상황',\n 324: '뭘',\n 325: '추천',\n 326: '지',\n 327: '매일',\n 328: '역시',\n 329: '드립니다',\n 330: '온',\n 331: '카드',\n 332: '자꾸',\n 333: '위로',\n 334: '을',\n 335: '힘든데',\n 336: '가네요',\n 337: '아픈가요',\n 338: '가까워질',\n 339: '버렸어',\n 340: '있어도',\n 341: '보면',\n 342: '눈물',\n 343: '랑',\n 344: 'ㅠㅠ',\n 345: '를',\n 346: '고민',\n 347: '물어봐서',\n 348: '모르고',\n 349: '필요하죠',\n 350: '관계',\n 351: '처럼',\n 352: '아는데',\n 353: '살까',\n 354: '꼈어',\n 355: '괜찮아요',\n 356: '이나',\n 357: '의',\n 358: '하자고',\n 359: '하',\n 360: '잊고',\n 361: '가자고',\n 362: '왔나',\n 363: '처음',\n 364: '봐요',\n 365: '갑자기',\n 366: '찌푸려지죠',\n 367: '절약',\n 368: '그럴',\n 369: '만들어',\n 370: '거지',\n 371: '가서',\n 372: '하는데',\n 373: '으로',\n 374: '가스',\n 375: '이야기',\n 376: '싶네요',\n 377: '알',\n 378: '돼',\n 379: '했어',\n 380: '만',\n 381: '옆',\n 382: '살쪄도',\n 383: '공적',\n 384: '먼저',\n 385: '편해요',\n 386: '좋아해주세요',\n 387: '진리',\n 388: '막',\n 389: '했잖아',\n 390: '때',\n 391: '믿어',\n 392: '좋아요',\n 393: '됨',\n 394: '새',\n 395: '애',\n 396: '좋은',\n 397: '기운',\n 398: '감미로운',\n 399: '병원',\n 400: '켜놓고',\n 401: '사세요',\n 402: '왜',\n 403: '볼까',\n 404: '어제',\n 405: '리지',\n 406: '가야',\n 407: '매력',\n 408: '정말',\n 409: '너무',\n 410: '취미',\n 411: '가장',\n 412: '언제나',\n 413: '정',\n 414: '패턴',\n 415: '이라니',\n 416: '거',\n 417: '곧',\n 418: '있을',\n 419: '건데',\n 420: '바빠서',\n 421: '좋다',\n 422: '부터',\n 423: '궁금해',\n 424: '만나지',\n 425: '해주세요',\n 426: '가출',\n 427: '키울',\n 428: '피',\n 429: '단',\n 430: '에',\n 431: '일',\n 432: '말까',\n 433: '개',\n 434: '가',\n 435: '들',\n 436: '애가',\n 437: '놓고',\n 438: '따뜻하게',\n 439: '감기',\n 440: '휴식',\n 441: '걸린',\n 442: '좋더라',\n 443: '해',\n 444: '지망',\n 445: '친구',\n 446: '자신',\n 447: '까',\n 448: '같이',\n 449: '같은',\n 450: '곳',\n 451: '집',\n 452: '살찐',\n 453: '비싼데'}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 인덱스 -> 단어\n",
    "# 문장을 인덱스로 변환하여 모델 입력으로 사용\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "def convert_text_to_index(sentences, vocabulary, type):\n",
    "    sentences_index = []\n",
    "\n",
    "    # 모든 문장에 대해서 반복\n",
    "    for sentence in sentences:\n",
    "        sentence_index = []\n",
    "\n",
    "        # 디코더 입력일 경우 맨 앞에 START태그 추가\n",
    "        if type==DECODER_INPUT:\n",
    "            sentence_index.extend([vocabulary[STA]])\n",
    "\n",
    "        # 문장의 단어들을 띄어쓰기로 분리\n",
    "        for word in sentence.split():\n",
    "            if vocabulary.get(word) is not None:\n",
    "                #사전에 있는 단어면 해당 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[word]])\n",
    "            else:\n",
    "                #사전에 없는 단어면 OOV인덱스 추가\n",
    "                sentence_index.extend([vocabulary[OOV]])\n",
    "        # 최대 길이 검사\n",
    "        if type == DECODER_TARGET:\n",
    "            # 디코더 목표일 경우 맨 뒤에 END태그 추가\n",
    "            if len(sentence_index) >= max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
    "            else:\n",
    "                sentence_index+=[vocabulary[END]]\n",
    "        else:\n",
    "            if len(sentence_index) > max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences]\n",
    "        \n",
    "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "        sentence_index+=(max_sequences-len(sentence_index))*[vocabulary[PAD]]\n",
    "\n",
    "        # 문장의 인덱스 배열을 추가\n",
    "        sentences_index.append(sentence_index)\n",
    "\n",
    "    return np.asarray(sentences_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 seq2seq는 디코더의 현재 출력이 디코더의 다음 입력으로 들어갑니다. 다만 학습에서는 굳이 이렇게 하지 않고 디코더 입력과 디코더 출력의 데이터를 각각 만듭니다.\n",
    "\n",
    "그러나 예측시에는 이런 방식이 불가능. 출력값을 미리 알지 못하기 때문에, 디코더 입력을 사전에 생성할 수가 없습니다. 이런 문제를 해결하기 위해 훈련 모델과 예측 모델을 따로 구성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([260, 182,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 인코더 입력 인덱스 변환\n",
    "x_encoder = convert_text_to_index(question, word_to_index, ENCODER_INPUT)\n",
    "\n",
    "# 첫번째 인코더 입력 출력(12시 땡)\n",
    "x_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  1, 279, 434,  57, 336,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 디코더 입력 인덱스 변환\n",
    "x_decoder = convert_text_to_index(answer, word_to_index, DECODER_INPUT)\n",
    "\n",
    "# 첫번째 디코더 입력 출력(START 하루 가 또 가네요)\n",
    "x_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "100"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# 디코더 목표 인덱스 변환\n",
    "y_decoder = convert_text_to_index(answer,word_to_index, DECODER_TARGET)\n",
    "\n",
    "# 첫 번재 디코더 목표 출력(하루 가 또 가네요 END)\n",
    "y_decoder[0] # 2가 end tag\n",
    "len(y_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(30, 454)\n(100, 30, 454)\n"
    }
   ],
   "source": [
    "# 원핫인코딩 초기화\n",
    "one_hot_data = np.zeros((len(y_decoder), max_sequences, len(words))) #(100,30,454)===>(batch, 문장 길이, 단어개수)\n",
    "\n",
    "# 디코더 목표를 원핫인코딩으로 변환 #DECODER_TARGET\n",
    "# 학습 시 입력은 인덱스이지만, 출력은 원핫인코딩 형식\n",
    "for i, sequence in enumerate(y_decoder):\n",
    "    for j, index in enumerate(sequence):\n",
    "        one_hot_data[i,j,index]=1\n",
    "\n",
    "# 디코더 목표 설정\n",
    "y_decoder = one_hot_data\n",
    "\n",
    "# 첫번째 디코더 목표 출력\n",
    "y_decoder[0]\n",
    "print(y_decoder[0].shape) #(30,454)\n",
    "print(y_decoder.shape)    #(100,30,454) (30,454)가 총 100개 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 입력과 디코더 입력은 임베딩 레이어에 들어가는 인덱스 배열입니다. 반면에 디코더 출력은 원핫인코딩 형식이어야 합니다. 디코더의 마지막 Dense레이어에서 Softmax로 나오기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tensor(\"A4_1/embedding_lookup/Identity_2:0\", shape=(None, None, 100), dtype=float32)\nModel: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nA1 (InputLayer)                 (None, None)         0                                            \n__________________________________________________________________________________________________\nA3 (InputLayer)                 (None, None)         0                                            \n__________________________________________________________________________________________________\nA2 (Embedding)                  (None, None, 100)    45400       A1[0][0]                         \n__________________________________________________________________________________________________\nA4 (Embedding)                  (None, None, 100)    45400       A3[0][0]                         \n__________________________________________________________________________________________________\nA5 (LSTM)                       [(None, 128), (None, 117248      A2[0][0]                         \n__________________________________________________________________________________________________\nA6 (LSTM)                       [(None, None, 128),  117248      A4[0][0]                         \n                                                                 A5[0][1]                         \n                                                                 A5[0][2]                         \n__________________________________________________________________________________________________\nA7 (Dense)                      (None, None, 454)    58566       A6[0][0]                         \n==================================================================================================\nTotal params: 383,862\nTrainable params: 383,862\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "# 훈련 모델 인코더 정의\n",
    "#-----------------------------------\n",
    "\n",
    "# 입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "encoder_inputs = layers.Input(shape=(None,),name='A1')    \n",
    "\n",
    "# 임베딩 레이어\n",
    "encoder_outputs = layers.Embedding(len(words), embedding_dim,name='A2')(encoder_inputs) #(454,100) #[(None,None,100)]\n",
    "\n",
    "# return_state가 True면 상태값 리턴\n",
    "# LSTM은 state_h와 state_c 2개의 상태 존재\n",
    "# recurrent_dropout은 state 삭제시키는 것\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(lstm_hidden_dim, dropout=0.1, recurrent_dropout=0.5, return_state=True,name='A5')(encoder_outputs) #[(None,128),None// parameters: 117,248(4*128((128+1(bias)+100(embedding_dim)))\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#-------------------------------------\n",
    "# 훈련 모델 디코더 정의\n",
    "#------------------------------------\n",
    "\n",
    "# 목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "decoder_inputs = layers.Input(shape=(None,),name='A3')\n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_embedding = layers.Embedding(len(words),embedding_dim,name='A4') #(454,100)\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "print(decoder_outputs)\n",
    "\n",
    "# 인코더와 달리 return_sequences를 True로 설정하여 모든 타임스텝 출력값 리턴\n",
    "# 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리가히 위함\n",
    "decoder_lstm  = layers.LSTM(lstm_hidden_dim, dropout=0.1, recurrent_dropout=0.5, return_state=True, return_sequences=True,name='A6')\n",
    "\n",
    "\n",
    "# initial_state를 인코더의 상태로 초기화\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "# 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_dense = layers.Dense(len(words),activation='softmax',name='A7')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#-------------------------------------------\n",
    "# 훈련 모델 정의\n",
    "#-------------------------------------------\n",
    "\n",
    "# 입력과 출력으로 함수형 API모델 생성\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 방법 설정\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 예제는 Sequential방식의 모델이었습니다. 이번에는 함수형 API모델 사용. 인코더와 디코더가 따로 분리되어야 하는데, 단순히 레이어를 추가하여 붙이는 순차형으로는 구현이 불가능\n",
    "\n",
    "Model()함수로 입력과 출력을 따로 설정하며 모델 만듭니다. 그 다음 compile과 fit은 이전과 동일하게 적용하시면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nA3 (InputLayer)                 (None, None)         0                                            \n__________________________________________________________________________________________________\nA4 (Embedding)                  (None, None, 100)    45400       A3[0][0]                         \n__________________________________________________________________________________________________\nB1 (InputLayer)                 (None, 128)          0                                            \n__________________________________________________________________________________________________\nB2 (InputLayer)                 (None, 128)          0                                            \n__________________________________________________________________________________________________\nA6 (LSTM)                       [(None, None, 128),  117248      A4[1][0]                         \n                                                                 B1[0][0]                         \n                                                                 B2[0][0]                         \n__________________________________________________________________________________________________\nA7 (Dense)                      (None, None, 454)    58566       A6[1][0]                         \n==================================================================================================\nTotal params: 221,214\nTrainable params: 221,214\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "#-----------------------------------\n",
    "#예측 모델 인코더 정의\n",
    "#-----------------------------------\n",
    "\n",
    "# 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정\n",
    "# encoder_states = [state_h, state_c]\n",
    "encoder_model = models.Model(input=encoder_inputs, output=encoder_states) # hidden상태값이 output\n",
    "\n",
    "#-----------------------------------\n",
    "# 예측 모델 디코더 정의\n",
    "#-----------------------------------\n",
    "\n",
    "# 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행\n",
    "# 매번 이전 디코더 상태를 입력으로 받아서 새로 설정\n",
    "decoder_state_input_h = layers.Input(shape=(lstm_hidden_dim,),name='B1')\n",
    "decoder_state_input_c = layers.Input(shape=(lstm_hidden_dim,),name='B2')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_outputs = decoder_embedding(decoder_inputs) # A4\n",
    "\n",
    "# LSTM레이어\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_outputs, initial_state = decoder_states_inputs) # A6\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# Dense레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 예측 모델 디코더 설정\n",
    "decoder_model = models.Model([decoder_inputs]+decoder_states_inputs, [decoder_outputs]+decoder_states)\n",
    "decoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 모델은 이미 학습된 훈련 모델의 레이어들을 그대로 재사용. 예측 모델 인코더는 훈련 모델 인코더와 동일. 그러나 예측 모델 디코더는 매번 LSTM상태값을 입력 받음. 또한 디코더의 LSTM상태를 출력값과 같이 내보내서, 다음 번 입력에 넣습니다.\n",
    "\n",
    "이렇게 하는 이유는 LSTM을 딱 한번의 타임스텝만 실행하기 때문. 그래서 매번 상태값을 새로 초기화 해야 합니다. 이와 반대로 훈련할 때는 문장 전체를 계속 LSTM으로 돌리기 때문에 자동으로 상태값이 전달됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 문장으롭 변환\n",
    "def convert_index_to_text(indexs, vocabulary):\n",
    "    sentence=''\n",
    "\n",
    "    #모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == END_INDEX:\n",
    "            #종료 인덱스면 중지\n",
    "            break\n",
    "        elif vocabulary.get(index) is not None:\n",
    "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "            sentence += vocabulary[index]\n",
    "        else:\n",
    "            sentence+=vocabulary[OOV_INDEX]\n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total Epoch: 1\naccuracy: 0.907\nloss: 0.42925808429718015\n저 도 잘 좋죠 \n\nTotal Epoch: 2\naccuracy: 0.96033335\nloss: 0.20186571896076203\n다음 은 또 가네요 \n\nTotal Epoch: 3\naccuracy: 0.97\nloss: 0.12371754139661789\n빨리 가 또 가네요 \n\nTotal Epoch: 4\naccuracy: 0.97\nloss: 0.10150670945644379\n빨리 가 또 가네요 \n\nTotal Epoch: 5\naccuracy: 0.9723333\nloss: 0.08988231152296067\n빨리 가 또 가네요 \n\nTotal Epoch: 6\naccuracy: 0.973\nloss: 0.08574528872966766\n빨리 가 또 가네요 \n\nTotal Epoch: 7\naccuracy: 0.975\nloss: 0.08002353608608245\n빨리 가 또 가네요 \n\nTotal Epoch: 8\naccuracy: 0.97466666\nloss: 0.07758412599563598\n빨리 가 또 가네요 \n\nTotal Epoch: 9\naccuracy: 0.976\nloss: 0.07296632677316665\n빨리 가 또 가네요 \n\nTotal Epoch: 10\naccuracy: 0.978\nloss: 0.06524469852447509\n빨리 가 또 가네요 \n\nTotal Epoch: 11\naccuracy: 0.9763333\nloss: 0.06479136750102044\n빨리 가 또 가네요 \n\nTotal Epoch: 12\naccuracy: 0.984\nloss: 0.048659982830286025\n빨리 가 또 가네요 \n\nTotal Epoch: 13\naccuracy: 0.9866667\nloss: 0.04122744008898735\n하루 가 또 가네요 \n\nTotal Epoch: 14\naccuracy: 0.988\nloss: 0.03593910783529282\n하루 가 또 가네요 \n\nTotal Epoch: 15\naccuracy: 0.9916667\nloss: 0.026472023352980614\n곧 가 또 가네요 \n\nTotal Epoch: 16\naccuracy: 0.9943333\nloss: 0.019647425934672356\n하루 가 또 가네요 \n\nTotal Epoch: 17\naccuracy: 0.9953333\nloss: 0.015692191570997237\n곧 가 또 가네요 \n\nTotal Epoch: 18\naccuracy: 0.9956667\nloss: 0.01511571042239666\n곧 가 또 가네요 \n\nTotal Epoch: 19\naccuracy: 0.997\nloss: 0.00900479596108198\n하루 가 또 가네요 \n\nTotal Epoch: 20\naccuracy: 0.99833333\nloss: 0.006647221501916647\n하루 가 또 가네요 \n\n"
    }
   ],
   "source": [
    "# epoch 반복\n",
    "for epoch in range(20):\n",
    "    print('Total Epoch:', epoch+1)\n",
    "\n",
    "    # 훈련 시작\n",
    "    history = model.fit([x_encoder,x_decoder], # input\n",
    "                        y_decoder,  # output\n",
    "                        epochs=100,\n",
    "                        batch_size=64,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # 정확도와 손실 출력\n",
    "    print('accuracy:',history.history['acc'][-1])\n",
    "    print('loss:',history.history['loss'][-1])\n",
    "\n",
    "    # 문장 예측 테스트\n",
    "    # (3박 4일 놀러 가고 싶다) -> (여행 은 언제나 좋죠)\n",
    "    input_encoder = x_encoder[0].reshape(1, x_encoder[2].shape[0]) # (1,30)\n",
    "    input_decoder = x_decoder[0].reshape(1, x_decoder[2].shape[0])\n",
    "    results = model.predict([input_encoder, input_decoder])\n",
    "\n",
    "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "    # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
    "    indexs = np.argmax(results[0],1)\n",
    "\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(30,)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "x_encoder[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 30)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "input_encoder = x_encoder[2].reshape(1,30)\n",
    "input_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(30, 454)\n(1, 30, 454)\n"
    }
   ],
   "source": [
    "results[0]\n",
    "print(results[0].shape) #(30,454)\n",
    "print(results.shape)    #(1,30,454)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 진행될수록 예측 문장이 제대로 생성되는 것을 볼 수 있다. 다만 여기서의 예측은 단순히 테스트를 위한 것이라, 인코더 입력과 디코더 입력 데이터가 동시에 사용. 아래 문장 생성에서는 예측 모델을 적용하기 때문에, 오직 인코더 입력 데이터만 집어 넣습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "encoder_model.save(r'D://PROJECT/model/seq2seq_chatbot_encoder_model.h5')\n",
    "decoder_model.save(r'D://PROJECT/model/seq2seq_chatbot_decoder_model.h5')\n",
    "\n",
    "# 인덱스 저장\n",
    "with open(r'D://PROJECT/model/word_to_index.pkl','wb')as f:\n",
    "    pickle.dump(word_to_index, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'D://PROJECT/model/index_to_word.pkl','wb')as f:\n",
    "    pickle.dump(index_to_word,f,pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pickle모듈><br>\n",
    "일반 텍스트를 파일로 저장할 떄는 파일 입출력 이용\n",
    "하지만 리스트나 클래스같은 텍스트가 아닌 자료형은 일반적인 파일 입출력 방법으로는 데이터를 저장하거나 불러올 수 없다. <br>\n",
    "pickle모듈을 이용하면 원하는 데이터를 자료형의 변경 없이 파일로 저장하여 그대로 로드할 수 있다. <br>\n",
    "pickle로 데이터를 저장하거나 불러올때는 파일을 바이트형식으로 읽거나 써야한다.(wb,rb)\n",
    "\n",
    "pickle.load()는 한줄씩 데이터를 읽어오고\n",
    "pickle.dump()는 뭉탱이로 읽어옴\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파일 로드\n",
    "encoder_model = models.load_model(r'D://PROJECT//model/seq2seq_chatbot_encoder_model.h5')\n",
    "decoder_model = models.load_model(r'D://PROJECT//model/seq2seq_chatbot_decoder_model.h5')\n",
    "\n",
    "# 인덱스 파일 로드\n",
    "with open(r'D://PROJECT//model/word_to_index.pkl','rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "with open(r'D://PROJECT//model/index_to_word.pkl','rb') as f:\n",
    "    index_to_word  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 입력 생성\n",
    "def make_predict_input(sentence):\n",
    "\n",
    "    sentences = []\n",
    "    sentences.append(sentence)\n",
    "    sentences = pos_tag(sentences) # 형태소 분석\n",
    "    input_seq = convert_text_to_index(sentences, word_to_index, ENCODER_INPUT) # 인덱스화\n",
    "\n",
    "    return input_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성\n",
    "def generate_text(input_seq):\n",
    "\n",
    "    # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    states = encoder_model.predict(input_seq) # 인덱스\n",
    "\n",
    "    # 목표 시퀀스 초기화\n",
    "    # 문장 1개, 토큰 1개\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "    target_seq[0,0] = STA_INDEX\n",
    "\n",
    "    # 인덱스 초기화\n",
    "    indexs = []\n",
    "\n",
    "    # 디코더 타임 스탭 반복\n",
    "    while 1:\n",
    "        # 디코더로 현재 타임 스텝 출력 구함\n",
    "        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "        decoder_outputs, state_h, state_c = decoder_model.predict([target_seq]+states) # start + encoder_state(문맥벡터)\n",
    "\n",
    "        # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "        index = np.argmax(decoder_outputs[0,0,:])\n",
    "        indexs.append(index)\n",
    "\n",
    "        # 종료 검사\n",
    "        if index == END_INDEX or len(indexs) >= max_sequences:\n",
    "            break\n",
    "        # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = index     # 다음 출력할 때 input_index\n",
    "\n",
    "        # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "        states = [state_h, state_c] # 다음 출력할 때 input_state\n",
    "\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제일 첫 단어는 START로 시작. 그리고 출력으로 나온 인덱스를 디코더 입력으로 넣고 다시 예측 반복. 상태값을 받아 다시 입력으로 같이 넣는 것에 주의. END태그가 나오면 문장 생성 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[445, 343,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0]])"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('친구랑 싸웠어요')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'저 도 싫어요 '"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[3241,  445,    3,    3,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0]])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('친구랑 심하게 싸웠어요')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'질문 자 님 이 엄마 때문 에 꿈 을 포기 했을 때 어떻게 해야 하는지 고민 이 되어 이렇게 고민 글 을 올려 주었네요 어떤 꿈이기에 포기 할 수 '"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# 예측 모델로 텍스트 생성\n",
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 문장에서는 없던 '같이'를 단어를 추가해 보았습니다. 그래도 비슷한 의미란 것을 파악하여 동일한 답변이 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[445, 343,   3, 151,   3,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0]])"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# 문장을 인덱스로 변환\n",
    "input_seq = make_predict_input('친구랑 싸울 것 같아요')\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'이럴 때 잘 쉬는 게 중요해요 '"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "sentence = generate_text(input_seq)\n",
    "sentence"
   ]
  }
 ]
}