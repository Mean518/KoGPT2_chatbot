{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597643053984",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence-to-sequence학습이란?\n",
    ">> 이 모델은 기계 번역 혹은 자유로운 질의응답에 사용\n",
    ">> 자연어 질문을 주어 자연어 응답을 생성\n",
    ">> 일반적으로, 텍스트를 생성해야 할 경우라면 언제든지 적용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'S': 0, 'E': 1, 'P': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '사': 29, '랑': 30, '얼': 31, '굴': 32, '주': 33, '택': 34, '희': 35, '망': 36, '나': 37, '무': 38, '바': 39, '위': 40}\n41\n"
    }
   ],
   "source": [
    "# 캐릭터 글자 목록\n",
    "# S -> Start, E -> End, P -> Padding\n",
    "char_list = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz사랑얼굴주택희망나무바위']\n",
    "\n",
    "# 캐릭터 사전 생성\n",
    "char_to_idx = {c: i for i, c in enumerate(char_list)}\n",
    "dic_len = len(char_to_idx)\n",
    "\n",
    "print(char_to_idx)\n",
    "print(dic_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 목록\n",
    "# 앞의 영어를 뒤의 한글로 번역\n",
    "word_list = [['love', '사랑'], ['face', '얼굴'],\n",
    "            ['home', '주택'], ['hope', '희망'],\n",
    "            ['tree', '나무'], ['rock', '바위']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = []\n",
    "decoder_input = []\n",
    "decoder_target = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(6, 4, 41)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "for words in word_list:\n",
    "    enc_input = [char_to_idx[c] for c in words[0]]\n",
    "    encoder_input.append(np.eye(dic_len)[enc_input])\n",
    "np.array(encoder_input).shape # 단어 6개, time_step=4, 단어사전길이:41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 데이터 생성\n",
    "def make_batch(word_list):\n",
    "    \n",
    "    encoder_input = []\n",
    "    decoder_input = []\n",
    "    decoder_target = []\n",
    "\n",
    "    for words in word_list:\n",
    "        # 인코더 입력 단어를 인덱스로 변환\n",
    "        # Ex) l,o,v,e\n",
    "        enc_input = [char_to_idx[c] for c in words[0]]\n",
    "\n",
    "        # 디코더 입력 단어를 인덱스로 변환\n",
    "        # 제일 앞에 시작 태그 삽입\n",
    "        # Ex) S,사,랑\n",
    "        dec_input = [char_to_idx[c] for c in ('S' + words[1])]\n",
    "        \n",
    "        # 디코더 목표 캐릭터를 인덱스로 변환\n",
    "        # 제일 끝에 종료 태그 삽입\n",
    "        # Ex) 사,랑,E\n",
    "        dec_target = [char_to_idx[c] for c in (words[1] + 'E')]\n",
    "\n",
    "        # 원핫인코딩으로 변환\n",
    "        encoder_input.append(np.eye(dic_len)[enc_input])\n",
    "        decoder_input.append(np.eye(dic_len)[dec_input])\n",
    "        decoder_target.append(np.eye(dic_len)[dec_target])\n",
    "\n",
    "    return np.array(encoder_input), np.array(decoder_input), np.array(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 생성\n",
    "x_encoder, x_decoder, y_decoder = make_batch(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "x_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x_decoder[0] # s, 사, 랑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y_decoder[0] # 사, 랑, E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "encoder_inputs = Input(shape=(None,dic_len)) # time_step None으로 받기, 문자 길이는 원래 정해져 있지 않음\n",
    "\n",
    "# return_state가 True면 상태값 리턴\n",
    "# return_state는 encoder의 출력과 내부 RNN상태인 리스트를 반환하도록 RNN을 구성하는 인수\n",
    "# 이는 encoder의 상태를 복구하는 데 사용\n",
    "# LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재\n",
    "encoder_outputs, state_h, state_c = LSTM(64,\n",
    "                                        dropout=0.1,\n",
    "                                        recurrent_dropout=0.5,\n",
    "                                        return_state=True)(encoder_inputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "# time_step = None\n",
    "decoder_inputs = Input(shape=(None,dic_len))\n",
    "\n",
    "# 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴\n",
    "# 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함\n",
    "decoder_lstm = LSTM(64,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.5,\n",
    "                    return_state=True,\n",
    "                    return_sequences=True)\n",
    "\n",
    "# initial_state를 인코더의 상태로 초기화\n",
    "# initial_state는 RNN의 초기 상태를 지정하는 인수. 초기 상태로 encoder를 decoder로 전달하는 데 사용\n",
    "# input은 decoder_inputs, encoder_states\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_dense = Dense(dic_len, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------\n",
    "# 모델 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력과 출력으로 함수형 API 모델 생성\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 방법 설정\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.6865 - acc: 0.1111\nEpoch 2/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.6201 - acc: 0.3333\nEpoch 3/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.5620 - acc: 0.3333\nEpoch 4/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.4649 - acc: 0.3333\nEpoch 5/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.3476 - acc: 0.3333\nEpoch 6/200\n6/6 [==============================] - 0s 2ms/step - loss: 3.1845 - acc: 0.3333\nEpoch 7/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.9562 - acc: 0.3333\nEpoch 8/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.5727 - acc: 0.3333\nEpoch 9/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.4364 - acc: 0.3333\nEpoch 10/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.3344 - acc: 0.3333\nEpoch 11/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.3004 - acc: 0.3333\nEpoch 12/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.2438 - acc: 0.3333\nEpoch 13/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.1974 - acc: 0.3333\nEpoch 14/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.1403 - acc: 0.3333\nEpoch 15/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.1231 - acc: 0.3333\nEpoch 16/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.0695 - acc: 0.3333\nEpoch 17/200\n6/6 [==============================] - 0s 2ms/step - loss: 2.0234 - acc: 0.3333\nEpoch 18/200\n6/6 [==============================] - 0s 3ms/step - loss: 2.0059 - acc: 0.3333\nEpoch 19/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.9555 - acc: 0.3333\nEpoch 20/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.9679 - acc: 0.3333\nEpoch 21/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.8797 - acc: 0.3333\nEpoch 22/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.8675 - acc: 0.3333\nEpoch 23/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.8529 - acc: 0.3333\nEpoch 24/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.7632 - acc: 0.3333\nEpoch 25/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.7463 - acc: 0.3889\nEpoch 26/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.7015 - acc: 0.3889\nEpoch 27/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.6726 - acc: 0.3889\nEpoch 28/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.6659 - acc: 0.4444\nEpoch 29/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.6370 - acc: 0.4444\nEpoch 30/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.5647 - acc: 0.4444\nEpoch 31/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.5303 - acc: 0.3889\nEpoch 32/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.5255 - acc: 0.4444\nEpoch 33/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.5067 - acc: 0.4444\nEpoch 34/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.5189 - acc: 0.3889\nEpoch 35/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.4577 - acc: 0.5556\nEpoch 36/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.3922 - acc: 0.5556\nEpoch 37/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.3566 - acc: 0.6667\nEpoch 38/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.3218 - acc: 0.5556\nEpoch 39/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.3408 - acc: 0.5556\nEpoch 40/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.3414 - acc: 0.6667\nEpoch 41/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.2545 - acc: 0.6667\nEpoch 42/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.1738 - acc: 0.7222\nEpoch 43/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.2736 - acc: 0.6111\nEpoch 44/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.1388 - acc: 0.8333\nEpoch 45/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.1102 - acc: 0.7778\nEpoch 46/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.1367 - acc: 0.7222\nEpoch 47/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0858 - acc: 0.7778\nEpoch 48/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.1495 - acc: 0.8333\nEpoch 49/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0925 - acc: 0.8333\nEpoch 50/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0133 - acc: 0.7778\nEpoch 51/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.9869 - acc: 0.8889\nEpoch 52/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0174 - acc: 0.8889\nEpoch 53/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0356 - acc: 0.7778\nEpoch 54/200\n6/6 [==============================] - 0s 2ms/step - loss: 1.0609 - acc: 0.6667\nEpoch 55/200\n1/6 [====>.........................] - ETA: 0s - loss: 0.7688 - acc: 1.0006/6 [==============================] - 0s 2ms/step - loss: 0.9004 - acc: 0.8333\nEpoch 56/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.9746 - acc: 0.8889\nEpoch 57/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.9658 - acc: 0.7778\nEpoch 58/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.8146 - acc: 0.9444\nEpoch 59/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.8061 - acc: 0.9444\nEpoch 60/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.8646 - acc: 0.8889\nEpoch 61/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.7805 - acc: 0.9444\nEpoch 62/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.8467 - acc: 0.8333\nEpoch 63/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.7749 - acc: 0.9444\nEpoch 64/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.7993 - acc: 0.9444\nEpoch 65/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.7762 - acc: 0.8889\nEpoch 66/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.7091 - acc: 0.8333\nEpoch 67/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6881 - acc: 0.8889\nEpoch 68/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.8631 - acc: 0.8333\nEpoch 69/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.7421 - acc: 0.8333\nEpoch 70/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.7751 - acc: 0.8333\nEpoch 71/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6291 - acc: 0.9444\nEpoch 72/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6277 - acc: 0.9444\nEpoch 73/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6938 - acc: 0.8889\nEpoch 74/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6676 - acc: 0.9444\nEpoch 75/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.5589 - acc: 0.9444\nEpoch 76/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6835 - acc: 0.8889\nEpoch 77/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.5635 - acc: 0.9444\nEpoch 78/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6640 - acc: 0.8333\nEpoch 79/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6066 - acc: 0.9444\nEpoch 80/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.5872 - acc: 0.9444\nEpoch 81/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.5429 - acc: 1.0000\nEpoch 82/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4297 - acc: 1.0000\nEpoch 83/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6564 - acc: 0.8333\nEpoch 84/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.6329 - acc: 0.8889\nEpoch 85/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4629 - acc: 1.0000\nEpoch 86/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.5991 - acc: 0.8333\nEpoch 87/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4319 - acc: 0.9444\nEpoch 88/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4650 - acc: 0.9444\nEpoch 89/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3566 - acc: 1.0000\nEpoch 90/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4402 - acc: 1.0000\nEpoch 91/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.4886 - acc: 0.9444\nEpoch 92/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4732 - acc: 1.0000\nEpoch 93/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4325 - acc: 0.9444\nEpoch 94/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4540 - acc: 0.8333\nEpoch 95/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4794 - acc: 0.8889\nEpoch 96/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.5204 - acc: 0.8889\nEpoch 97/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3873 - acc: 0.9444\nEpoch 98/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4539 - acc: 1.0000\nEpoch 99/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3674 - acc: 0.9444\nEpoch 100/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4389 - acc: 1.0000\nEpoch 101/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4480 - acc: 0.8889\nEpoch 102/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3967 - acc: 1.0000\nEpoch 103/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3244 - acc: 1.0000\nEpoch 104/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4113 - acc: 0.9444\nEpoch 105/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4406 - acc: 0.8889\nEpoch 106/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3137 - acc: 1.0000\nEpoch 107/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4025 - acc: 0.9444\nEpoch 108/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2945 - acc: 1.0000\nEpoch 109/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2847 - acc: 0.9444\nEpoch 110/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3502 - acc: 1.0000\nEpoch 111/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3035 - acc: 1.0000\nEpoch 112/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.3705 - acc: 1.0000\nEpoch 113/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2913 - acc: 1.0000\nEpoch 114/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2634 - acc: 1.0000\nEpoch 115/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2696 - acc: 1.0000\nEpoch 116/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2590 - acc: 1.0000\nEpoch 117/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3715 - acc: 0.9444\nEpoch 118/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3037 - acc: 0.9444\nEpoch 119/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2223 - acc: 1.0000\nEpoch 120/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2272 - acc: 1.0000\nEpoch 121/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2717 - acc: 0.9444\nEpoch 122/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2447 - acc: 1.0000\nEpoch 123/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2281 - acc: 1.0000\nEpoch 124/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2382 - acc: 1.0000\nEpoch 125/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.1802 - acc: 1.0000\nEpoch 126/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2553 - acc: 0.9444\nEpoch 127/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3172 - acc: 0.9444\nEpoch 128/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2102 - acc: 1.0000\nEpoch 129/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1564 - acc: 1.0000\nEpoch 130/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1782 - acc: 1.0000\nEpoch 131/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1864 - acc: 1.0000\nEpoch 132/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1827 - acc: 0.9444\nEpoch 133/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1513 - acc: 1.0000\nEpoch 134/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3409 - acc: 0.9444\nEpoch 135/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2845 - acc: 1.0000\nEpoch 136/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2662 - acc: 1.0000\nEpoch 137/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1725 - acc: 1.0000\nEpoch 138/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2240 - acc: 0.9444\nEpoch 139/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.1992 - acc: 1.0000\nEpoch 140/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1931 - acc: 1.0000\nEpoch 141/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1677 - acc: 1.0000\nEpoch 142/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1746 - acc: 1.0000\nEpoch 143/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3007 - acc: 0.8889\nEpoch 144/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1431 - acc: 1.0000\nEpoch 145/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1411 - acc: 1.0000\nEpoch 146/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2830 - acc: 0.8889\nEpoch 147/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2035 - acc: 1.0000\nEpoch 148/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1033 - acc: 1.0000\nEpoch 149/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2327 - acc: 0.9444\nEpoch 150/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.1197 - acc: 1.0000\nEpoch 151/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2044 - acc: 0.9444\nEpoch 152/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.1537 - acc: 1.0000\nEpoch 153/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1599 - acc: 0.9444\nEpoch 154/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1256 - acc: 1.0000\nEpoch 155/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0987 - acc: 1.0000\nEpoch 156/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1429 - acc: 1.0000\nEpoch 157/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.4026 - acc: 0.8333\nEpoch 158/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1140 - acc: 1.0000\nEpoch 159/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0989 - acc: 1.0000\nEpoch 160/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2142 - acc: 0.9444\nEpoch 161/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1194 - acc: 1.0000\nEpoch 162/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1158 - acc: 1.0000\nEpoch 163/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1093 - acc: 1.0000\nEpoch 164/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0934 - acc: 1.0000\nEpoch 165/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1520 - acc: 0.9444\nEpoch 166/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1095 - acc: 1.0000\nEpoch 167/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2213 - acc: 0.9444\nEpoch 168/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0985 - acc: 1.0000\nEpoch 169/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1004 - acc: 1.0000\nEpoch 170/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1136 - acc: 1.0000\nEpoch 171/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1248 - acc: 1.0000\nEpoch 172/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1219 - acc: 1.0000\nEpoch 173/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0657 - acc: 1.0000\nEpoch 174/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0522 - acc: 1.0000\nEpoch 175/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.2123 - acc: 0.9444\nEpoch 176/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1044 - acc: 0.9444\nEpoch 177/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0976 - acc: 1.0000\nEpoch 178/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1519 - acc: 0.9444\nEpoch 179/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0638 - acc: 1.0000\nEpoch 180/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1076 - acc: 1.0000\nEpoch 181/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0957 - acc: 1.0000\nEpoch 182/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.3853 - acc: 0.8333\nEpoch 183/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - acc: 1.0000\nEpoch 184/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0496 - acc: 1.0000\nEpoch 185/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1020 - acc: 1.0000\nEpoch 186/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1825 - acc: 0.9444\nEpoch 187/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0668 - acc: 1.0000\nEpoch 188/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0770 - acc: 1.0000\nEpoch 189/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0977 - acc: 1.0000\nEpoch 190/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0848 - acc: 1.0000\nEpoch 191/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0438 - acc: 1.0000\nEpoch 192/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1660 - acc: 0.8889\nEpoch 193/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0435 - acc: 1.0000\nEpoch 194/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0914 - acc: 1.0000\nEpoch 195/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0293 - acc: 1.0000\nEpoch 196/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.1497 - acc: 1.0000\nEpoch 197/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - acc: 1.0000\nEpoch 198/200\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - acc: 1.0000\nEpoch 199/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0454 - acc: 1.0000\nEpoch 200/200\n6/6 [==============================] - 0s 2ms/step - loss: 0.0501 - acc: 1.0000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2566114cd48>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "model.fit([x_encoder, x_decoder],\n",
    "           y_decoder,\n",
    "           epochs=200,\n",
    "           batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 수행\n",
    "def translate(word):\n",
    "    \n",
    "    # 영어/한글 배열 생성\n",
    "    # 한글은 정답을 모르기 때문에 패딩으로 채움\n",
    "    words = [word, 'P' * len(word)]\n",
    "\n",
    "    # 배치 데이터 생성\n",
    "    x_encoder, x_decoder, y_decoder = make_batch([words])\n",
    "    \n",
    "    # 예측 수행\n",
    "    # 원핫인코딩으로 결과 나옴\n",
    "    results = model.predict([x_encoder, x_decoder])\n",
    "\n",
    "    # 2축을 기준으로 최대값의 인덱스 구함\n",
    "    results = np.argmax(results, 2) \n",
    "\n",
    "    # 인덱스를 캐릭터로 변환\n",
    "    decoded = [char_list[i] for i in results[0]]\n",
    "\n",
    "    # 종료 태그인 'E' 이후의 글자들을 제거하고 문자열 생성\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "words = ['love','P'*len('word')]\n",
    "x_encoder, x_decoder, y_decoder = make_batch([words])\n",
    "y_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[29, 30,  1,  1,  1]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "results = model.predict([x_encoder,x_decoder])\n",
    "results = np.argmax(results,2)\n",
    "results # 사, 랑 , E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_encoder = 'love'<br>\n",
    "x_decoder = 'SPPPP'<br>\n",
    "y_decoder = 'PPPPE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('love ->', translate('love'))\n",
    "print('lovi ->', translate('lovi'))\n",
    "print('litr ->', translate('litr'))\n",
    "print('hope ->', translate('hope'))\n",
    "print('hopu ->', translate('hopu'))\n",
    "print('hufe ->', translate('hufe'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}